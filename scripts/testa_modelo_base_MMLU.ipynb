{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whxyZm5LsSU6"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \\\n",
        "  \"bitsandbytes==0.46.0\" \\\n",
        "  \"transformers==4.41.2\" \\\n",
        "  \"peft==0.11.1\" \\\n",
        "  \"accelerate==0.31.0\" \\\n",
        "  \"datasets==2.19.2\" \\\n",
        "  \"trl==0.8.6\" \\\n",
        "  \"huggingface_hub\" \\\n",
        "  \"minijinja\" \\\n",
        "  \"triton==3.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login\n",
        "import os"
      ],
      "metadata": {
        "id": "BxR4aKdVsYG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def fix_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "8YS7ciu4sZKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "oflB8pKvsaSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID_FINETUNE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "NEW_MODEL_NAME = f\"{MODEL_ID_FINETUNE.split('/')[-1]}-synthetic-qlora\"\n",
        "OUTPUT_DIR = f\"./{NEW_MODEL_NAME}-results\"\n",
        "\n",
        "print(f\"Modelo base para fine-tuning: {MODEL_ID_FINETUNE}\")\n",
        "print(f\"Nome do novo adaptador: {NEW_MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "wRzKq7Nnsc_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Configuração de Quantização\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",       # Tipo de quantização em NormalFloat 4\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Dtype para cômputo (float16, bfloat16)\n",
        "    bnb_4bit_use_double_quant=False, # Usar quantização dupla (economiza um pouco mais de memória)\n",
        ")"
      ],
      "metadata": {
        "id": "6eZEMKWZsd_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Carregando modelo base: {MODEL_ID_FINETUNE}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID_FINETUNE,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "print(\"Modelo carregado.\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID_FINETUNE, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Tokenizer carregado.\")\n",
        "print(f\"PAD token ID: {tokenizer.pad_token_id}, EOS token ID: {tokenizer.eos_token_id}\")"
      ],
      "metadata": {
        "id": "R5CtoQiqse-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Lista dos arquivos JSON da base MMLU\n",
        "MMLU_INPUT_FILES = [\n",
        "    'mmlu_data_1.json',\n",
        "    'mmlu_data_2.json',\n",
        "    'mmlu_data_3.json'\n",
        "]\n",
        "\n",
        "# Nome do arquivo para salvar os resultados da avaliação MMLU\n",
        "MMLU_OUTPUT_FILE = 'mmlu_evaluation_results.jsonl'\n",
        "\n",
        "# Template do Prompt\n",
        "#    Este template inclui o texto inicial e a estrutura para os exemplos \"few-shot\".\n",
        "MMLU_PROMPT_TEMPLATE = \"\"\"The following are multiple choice questions (with answers) about various subjects. Choose the single most likely answer.\n",
        "\n",
        "--- BEGIN EXAMPLES ---\n",
        "\n",
        "(INSTRUCTION: Please replace the placeholders below with 4 high-quality, diverse examples to guide the model.)\n",
        "\n",
        "[FEW-SHOT EXAMPLE 1]\n",
        "Question: Which of the following is a type of sedimentary rock?\n",
        "Choices:\n",
        "A. Granite\n",
        "B. Marble\n",
        "C. Sandstone\n",
        "D. Slate\n",
        "Answer: C\n",
        "\n",
        "[FEW-SHOT EXAMPLE 2]\n",
        "Question: What is the capital of Japan?\n",
        "Choices:\n",
        "A. Beijing\n",
        "B. Seoul\n",
        "C. Tokyo\n",
        "D. Bangkok\n",
        "Answer: C\n",
        "\n",
        "[FEW-SHOT EXAMPLE 3]\n",
        "Question: Solve for x: 2x + 3 = 7\n",
        "Choices:\n",
        "A. 1\n",
        "B. 2\n",
        "C. 3\n",
        "D. 5\n",
        "Answer: B\n",
        "\n",
        "[FEW-SHOT EXAMPLE 4]\n",
        "Question: Who wrote \"Hamlet\"?\n",
        "Choices:\n",
        "A. Charles Dickens\n",
        "B. William Shakespeare\n",
        "C. Leo Tolstoy\n",
        "D. Mark Twain\n",
        "Answer: B\n",
        "\n",
        "--- END EXAMPLES ---\n",
        "\n",
        "Now, solve the following question. Provide only the letter of the correct answer.\n",
        "\n",
        "Question: {question}\n",
        "Choices:\n",
        "{choices}\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"Configurações da avaliação MMLU definidas.\")"
      ],
      "metadata": {
        "id": "-f0Z_Qh0shpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def evaluate_mmlu(model,\n",
        "                  tokenizer,\n",
        "                  input_files,\n",
        "                  output_file,\n",
        "                  prompt_template,\n",
        "                  max_new_tokens=5):\n",
        "    \"\"\"\n",
        "    Avalia o modelo em questões da base MMLU a partir de arquivos JSON locais.\n",
        "\n",
        "    Args:\n",
        "        model: O modelo a ser testado.\n",
        "        tokenizer: O tokenizer correspondente.\n",
        "        input_files (list): Lista de caminhos para os arquivos JSON de entrada.\n",
        "        output_file (str): Arquivo onde os resultados serão salvos.\n",
        "        prompt_template (str): O template do prompt com os exemplos few-shot.\n",
        "        max_new_tokens (int): Número máximo de tokens para a resposta (5 é suficiente para \"A\", \"B\", etc.).\n",
        "    \"\"\"\n",
        "    print(f\"Iniciando avaliação MMLU...\")\n",
        "\n",
        "    all_questions = []\n",
        "    # Abre cada um dos arquivos JSON e agrega todas as questões em uma única lista\n",
        "    for file_path in input_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                all_questions.extend(data)\n",
        "                print(f\" - Carregadas {len(data)} questões de '{file_path}'\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"AVISO: Arquivo '{file_path}' não encontrado. Pulando.\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"AVISO: Arquivo '{file_path}' não é um JSON válido. Pulando.\")\n",
        "\n",
        "    if not all_questions:\n",
        "        print(\"Nenhuma questão foi carregada. Abortando a avaliação.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total de {len(all_questions)} questões para avaliar.\")\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "        print(f\"Arquivo de log antigo '{output_file}' removido.\")\n",
        "\n",
        "    # Loop principal de avaliação\n",
        "    for i, item in enumerate(tqdm(all_questions, desc=\"Avaliando MMLU\")):\n",
        "        try:\n",
        "            question = item['question']\n",
        "            choices = item['choices']\n",
        "            correct_answer_index = item.get('answer', -1)\n",
        "\n",
        "            # Formata as opções de múltipla escolha\n",
        "            formatted_choices = \"\\n\".join([f\"{chr(65+j)}. {choice}\" for j, choice in enumerate(choices)])\n",
        "\n",
        "\n",
        "            final_prompt = prompt_template.format(\n",
        "                question=question,\n",
        "                choices=formatted_choices\n",
        "            )\n",
        "\n",
        "            inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "            # Decodifica apenas a resposta gerada\n",
        "            generated_answer = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "            result_log = {\n",
        "                \"index\": i,\n",
        "                \"subject\": item.get('subject', 'N/A'),\n",
        "                \"question\": question,\n",
        "                \"choices\": choices,\n",
        "                \"correct_answer\": choices[correct_answer_index] if correct_answer_index != -1 else \"N/A\",\n",
        "                \"correct_answer_letter\": chr(65 + correct_answer_index) if correct_answer_index != -1 else \"N/A\",\n",
        "                \"generated_answer\": generated_answer\n",
        "            }\n",
        "\n",
        "            # Salva o resultado de forma robusta a cada iteração\n",
        "            with open(output_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(json.dumps(result_log, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nErro ao processar a questão {i}. Erro: {e}\")\n",
        "            error_log = {\"index\": i, \"error\": str(e)}\n",
        "            with open(output_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(json.dumps(error_log, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"\\n🎉 Avaliação MMLU concluída! Resultados salvos em '{output_file}'.\")"
      ],
      "metadata": {
        "id": "_8dZEQHosjPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chama a função de avaliação MMLU, passando o modelo base já carregado\n",
        "evaluate_mmlu(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    input_files=MMLU_INPUT_FILES,\n",
        "    output_file=MMLU_OUTPUT_FILE,\n",
        "    prompt_template=MMLU_PROMPT_TEMPLATE\n",
        ")"
      ],
      "metadata": {
        "id": "jHdSGLQVskdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}