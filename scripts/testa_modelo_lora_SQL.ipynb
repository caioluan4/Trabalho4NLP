{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \\\n",
        "  \"bitsandbytes==0.46.0\" \\\n",
        "  \"transformers==4.41.2\" \\\n",
        "  \"peft==0.11.1\" \\\n",
        "  \"accelerate==0.31.0\" \\\n",
        "  \"datasets==2.19.2\" \\\n",
        "  \"trl==0.8.6\" \\\n",
        "  \"huggingface_hub\" \\\n",
        "  \"minijinja\" \\\n",
        "  \"triton==3.2.0\""
      ],
      "metadata": {
        "id": "bPGUAwCep4vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def fix_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "PZ4a17Fhp6dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "0XVWST-9p_Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip Meta-Llama-3-8B-Instruct-synthetic-qlora-results/Llama3-8B_Qlora1.zip\n",
        "\n",
        "print(\"✅ Arquivo descompactado com sucesso!\")"
      ],
      "metadata": {
        "id": "FpxXizpUthlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYTM2r-ooqmP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Parâmetros de Configuração\n",
        "\n",
        "# O ID do modelo base original\n",
        "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# O caminho para a pasta do seu adaptador salvo.\n",
        "ADAPTER_PATH = \"Meta-Llama-3-8B-Instruct-synthetic-qlora-results/Llama3-8B_Qlora1\"\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Carregando o modelo base: {base_model_id}\")\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"\\nCarregando o adaptador LoRA de: {ADAPTER_PATH}\")\n",
        "\n",
        "try:\n",
        "    # Carrega o adaptador e o acopla ao modelo base\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "\n",
        "    print(\"\\nOtimizando o modelo para inferência...\")\n",
        "\n",
        "    finetuned_model = finetuned_model.merge_and_unload()\n",
        "\n",
        "    print(\"✅ Modelo fine-tuned pronto para avaliação!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erro ao carregar o adaptador. Verifique se o caminho '{ADAPTER_PATH}' está correto.\")\n",
        "    print(f\"   Erro: {e}\")\n",
        "    finetuned_model = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Parâmetros de Configuração da Avaliação\n",
        "\n",
        "\n",
        "DEV_FILE = 'dev_formatted.jsonl'\n",
        "\n",
        "# Nome do arquivo para salvar os resultados do modelo fine-tuned\n",
        "LOG_FILE_FINETUNED_MODEL = 'evaluation_results_finetuned_model.jsonl'\n",
        "\n",
        "# Quantidade de exemplos para testar\n",
        "NUM_EXAMPLES_TO_TEST = 200\n",
        "\n",
        "if finetuned_model:\n",
        "    print(\"\\n--- Iniciando avaliação do MODELO FINE-TUNED ---\")\n",
        "    evaluate_model(\n",
        "        model=finetuned_model,\n",
        "        tokenizer=tokenizer,\n",
        "        dev_dataset_path=DEV_FILE,\n",
        "        output_log_file=LOG_FILE_FINETUNED_MODEL,\n",
        "        num_examples=NUM_EXAMPLES_TO_TEST\n",
        "    )\n",
        "else:\n",
        "    print(\"\\nA variável 'finetuned_model' não foi carregada. A avaliação foi abortada.\")"
      ],
      "metadata": {
        "id": "5gO64ym0q-gA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}