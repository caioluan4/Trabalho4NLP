{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75DWz8nJpEq8"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \\\n",
        "  \"bitsandbytes==0.46.0\" \\\n",
        "  \"transformers==4.41.2\" \\\n",
        "  \"peft==0.11.1\" \\\n",
        "  \"accelerate==0.31.0\" \\\n",
        "  \"datasets==2.19.2\" \\\n",
        "  \"trl==0.8.6\" \\\n",
        "  \"huggingface_hub\" \\\n",
        "  \"minijinja\" \\\n",
        "  \"triton==3.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def fix_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "iU-ckyfsr1BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "TVHi1ZnQr3Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip Meta-Llama-3-8B-Instruct-synthetic-qlora-results/Llama3-8B_Qlora1.zip\n",
        "\n",
        "print(\"‚úÖ Arquivo descompactado com sucesso!\")"
      ],
      "metadata": {
        "id": "Ts731FqVtcO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Par√¢metros de Configura√ß√£o\n",
        "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "ADAPTER_PATH = \"Meta-Llama-3-8B-Instruct-synthetic-qlora-results/Llama3-8B_Qlora1\"\n",
        "\n",
        "\n",
        "print(f\"Carregando o modelo base: {base_model_id}\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(f\"\\nCarregando o adaptador LoRA de: {ADAPTER_PATH}\")\n",
        "\n",
        "try:\n",
        "    finetuned_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "    finetuned_model = finetuned_model.merge_and_unload()\n",
        "    print(\"‚úÖ Modelo fine-tuned pronto para avalia√ß√£o!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro ao carregar o adaptador: {e}\")\n",
        "    finetuned_model = None"
      ],
      "metadata": {
        "id": "7r_SnHmXr7gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "# Lista dos seus arquivos JSON da base MMLU\n",
        "MMLU_INPUT_FILES = [\n",
        "    'mmlu_data_1.json',\n",
        "    'mmlu_data_2.json',\n",
        "    'mmlu_data_3.json'\n",
        "]\n",
        "\n",
        "# Nome do arquivo para salvar os resultados do modelo FINE-TUNED\n",
        "MMLU_FT_OUTPUT_FILE = 'mmlu_evaluation_results_finetuned.jsonl'\n",
        "\n",
        "# Template do Prompt\n",
        "MMLU_PROMPT_TEMPLATE = \"\"\"The following are multiple choice questions (with answers) about various subjects. Choose the single most likely answer.\n",
        "\n",
        "--- BEGIN EXAMPLES ---\n",
        "[FEW-SHOT EXAMPLE 1]\n",
        "Question: Which of the following is a type of sedimentary rock?\n",
        "Choices:\n",
        "A. Granite\n",
        "B. Marble\n",
        "C. Sandstone\n",
        "D. Slate\n",
        "Answer: C\n",
        "[FEW-SHOT EXAMPLE 2]\n",
        "Question: What is the capital of Japan?\n",
        "Choices:\n",
        "A. Beijing\n",
        "B. Seoul\n",
        "C. Tokyo\n",
        "D. Bangkok\n",
        "Answer: C\n",
        "[FEW-SHOT EXAMPLE 3]\n",
        "Question: Solve for x: 2x + 3 = 7\n",
        "Choices:\n",
        "A. 1\n",
        "B. 2\n",
        "C. 3\n",
        "D. 5\n",
        "Answer: B\n",
        "[FEW-SHOT EXAMPLE 4]\n",
        "Question: Who wrote \"Hamlet\"?\n",
        "Choices:\n",
        "A. Charles Dickens\n",
        "B. William Shakespeare\n",
        "C. Leo Tolstoy\n",
        "D. Mark Twain\n",
        "Answer: B\n",
        "--- END EXAMPLES ---\n",
        "\n",
        "Now, solve the following question. Provide only the letter of the correct answer.\n",
        "\n",
        "Question: {question}\n",
        "Choices:\n",
        "{choices}\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "def evaluate_mmlu(model, tokenizer, input_files, output_file, prompt_template, max_new_tokens=5):\n",
        "    \"\"\"Avalia o modelo em quest√µes da base MMLU a partir de arquivos JSON locais.\"\"\"\n",
        "    print(f\"Iniciando avalia√ß√£o MMLU...\")\n",
        "\n",
        "    all_questions = []\n",
        "    for file_path in input_files:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                all_questions.extend(data)\n",
        "                print(f\" - Carregadas {len(data)} quest√µes de '{file_path}'\")\n",
        "        except FileNotFoundError: print(f\"AVISO: Arquivo '{file_path}' n√£o encontrado. Pulando.\")\n",
        "        except json.JSONDecodeError: print(f\"AVISO: Arquivo '{file_path}' n√£o √© um JSON v√°lido. Pulando.\")\n",
        "\n",
        "    if not all_questions:\n",
        "        print(\"Nenhuma quest√£o foi carregada. Abortando a avalia√ß√£o.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total de {len(all_questions)} quest√µes para avaliar.\")\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "        print(f\"Arquivo de log antigo '{output_file}' removido.\")\n",
        "\n",
        "    for i, item in enumerate(tqdm(all_questions, desc=\"Avaliando MMLU\")):\n",
        "        try:\n",
        "            question, choices = item['question'], item['choices']\n",
        "            correct_answer_index = item.get('answer', -1)\n",
        "            formatted_choices = \"\\n\".join([f\"{chr(65+j)}. {choice}\" for j, choice in enumerate(choices)])\n",
        "            final_prompt = prompt_template.format(question=question, choices=formatted_choices)\n",
        "            inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "            generated_answer = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "            result_log = {\n",
        "                \"index\": i, \"subject\": item.get('subject', 'N/A'), \"question\": question,\n",
        "                \"choices\": choices, \"correct_answer_letter\": chr(65 + correct_answer_index) if correct_answer_index != -1 else \"N/A\",\n",
        "                \"generated_answer\": generated_answer\n",
        "            }\n",
        "\n",
        "            with open(output_file, 'a', encoding='utf-8') as f:\n",
        "                f.write(json.dumps(result_log, ensure_ascii=False) + '\\n')\n",
        "        except Exception as e:\n",
        "            print(f\"\\nErro ao processar a quest√£o {i}. Erro: {e}\")\n",
        "            error_log = {\"index\": i, \"error\": str(e)}\n",
        "            with open(output_file, 'a', encoding='utf-8') as f: f.write(json.dumps(error_log, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"\\nüéâ Avalia√ß√£o MMLU conclu√≠da! Resultados salvos em '{output_file}'.\")\n",
        "\n",
        "\n",
        "\n",
        "if finetuned_model:\n",
        "    print(\"\\n--- Iniciando avalia√ß√£o MMLU no MODELO FINE-TUNED ---\")\n",
        "    evaluate_mmlu(\n",
        "        model=finetuned_model,\n",
        "        tokenizer=tokenizer,\n",
        "        input_files=MMLU_INPUT_FILES,\n",
        "        output_file=MMLU_FT_OUTPUT_FILE,\n",
        "        prompt_template=MMLU_PROMPT_TEMPLATE\n",
        "    )\n",
        "else:\n",
        "    print(\"\\nA vari√°vel 'finetuned_model' n√£o foi carregada. A avalia√ß√£o MMLU foi abortada.\")"
      ],
      "metadata": {
        "id": "qhEKLP6sr-S-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}